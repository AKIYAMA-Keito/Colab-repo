{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKIYAMA-Keito/Colab-repo/blob/main/AWP_def.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d1Z8uIInOldG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKEv8KiMOldH"
      },
      "source": [
        "Step1 Tensorflowチュートリアル4から引用"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "version2\n"
      ],
      "metadata": {
        "id": "Yk9ofpRIRw2k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrpxk0rOldI"
      },
      "source": [
        "データセットを読み込んで正規化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QobaZNJyOldJ"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "random_index_train = np.array(range(len(x_train)))\n",
        "np.random.shuffle(random_index_train)\n",
        "x_train = x_train[random_index_train][:1000]\n",
        "y_train = y_train[random_index_train][:1000]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNcHvWE9OldJ"
      },
      "source": [
        "データセットをシャッフルしてバッチ化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wg2lQwV_OldJ"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)\n",
        ").batch(1000)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_test, y_test)\n",
        ").batch(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exQHK9sMOldK"
      },
      "source": [
        "CNNモデルを定義しインスタンスを取り出す．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1-UIMbDyOldK"
      },
      "outputs": [],
      "source": [
        "class CNNModel(Model):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = Conv2D(32, 3, activation = \"relu\")\n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(128, activation = \"relu\")\n",
        "        self.d2 = Dense(10)\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R46fxEjOOldK"
      },
      "source": [
        "損失関数とoptmizerを選択する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9pzC_gytOldL"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph_DSqV9OldL"
      },
      "source": [
        "損失関数とoptimizerの尺度評価のための関数を導入する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MipedjyLOldL"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6sZQRdOldL"
      },
      "source": [
        "モデルを訓練するための関数train_stepを定義する．\n",
        "予測値と正解ラベルの間の損失関数の勾配を最適化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KCmbB-cnOldL"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training = True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdL3GJY2OldM"
      },
      "source": [
        "モデルをテストするための関数test_stepを定義する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IsRpIQFEOldM"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "    predictions = model(images, training = False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "    \n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D-PUsW-OldM"
      },
      "source": [
        "学習を実行してテストし，結果を出力する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJEwX2bUOldM",
        "outputId": "4cc5f6f6-08de-49e1-b8e6-2fca0b55f91a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3113903999328613, Accuracy: 13.699999809265137, Test Loss: 1.9206812381744385, Test Accuracy: 47.279998779296875\n",
            "Epoch 2, Loss: 1.8868125677108765, Accuracy: 50.900001525878906, Test Loss: 1.4652187824249268, Test Accuracy: 70.22000122070312\n",
            "Epoch 3, Loss: 1.4113596677780151, Accuracy: 73.79999542236328, Test Loss: 1.1213607788085938, Test Accuracy: 78.22000122070312\n",
            "Epoch 4, Loss: 1.0528587102890015, Accuracy: 81.9000015258789, Test Loss: 0.8771098852157593, Test Accuracy: 81.20999908447266\n",
            "Epoch 5, Loss: 0.7971623539924622, Accuracy: 85.69999694824219, Test Loss: 0.7082427740097046, Test Accuracy: 82.80000305175781\n",
            "Epoch 6, Loss: 0.6204497218132019, Accuracy: 86.19999694824219, Test Loss: 0.597994863986969, Test Accuracy: 83.7300033569336\n",
            "Epoch 7, Loss: 0.5003252029418945, Accuracy: 87.69999694824219, Test Loss: 0.5253411531448364, Test Accuracy: 84.93000030517578\n",
            "Epoch 8, Loss: 0.4138632118701935, Accuracy: 89.9000015258789, Test Loss: 0.4817371368408203, Test Accuracy: 85.97000122070312\n",
            "Epoch 9, Loss: 0.3540738523006439, Accuracy: 91.19999694824219, Test Loss: 0.450300931930542, Test Accuracy: 86.5999984741211\n",
            "Epoch 10, Loss: 0.30922967195510864, Accuracy: 92.0999984741211, Test Loss: 0.4232850968837738, Test Accuracy: 87.45999908447266\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    \n",
        "    for train_images, train_labels in train_ds:\n",
        "        train_step(train_images, train_labels)\n",
        "    \n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_step(test_images, test_labels)\n",
        "    \n",
        "    print(\n",
        "        f'Epoch {epoch + 1}, '\n",
        "        f'Loss: {train_loss.result()}, '\n",
        "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "        f'Test Loss: {test_loss.result()}, '\n",
        "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AWPのための関数"
      ],
      "metadata": {
        "id": "2H3oxHw4D5U8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPKvxUYsOldM"
      },
      "source": [
        "Step2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def adversary(dataset, additive_weights, eps1 = 1, eta1 = 0.1, iterate_num = 5):\n",
        "    weights = model.get_weights()\n",
        "    new_weights = []\n",
        "    for w, v in zip(weights, additive_weights):\n",
        "      new_weights.append(w + v)\n",
        "    model.set_weights(new_weights)\n",
        "\n",
        "    adversarial_image_list = []\n",
        "    for (images, labels) in dataset:\n",
        "      for (image, label) in zip(images, labels):\n",
        "        image = tf.Variable([image])\n",
        "\n",
        "        initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "        image_dashed = tf.add(image, initial_noise)\n",
        "\n",
        "        # この書き方でも，initial_noiseが一様に入ってしまっている．\n",
        "        # tf.Variableで1次元追加しているから，より内側のループを作らないといけないよ\n",
        "        # image_dashed_list = []\n",
        "        # for image_0 in image:\n",
        "        #   for image_h in image_0:\n",
        "        #     for image_v in image_h:\n",
        "        #       for image_pixel in image_v:\n",
        "        #         initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "        #         image_pixel_dashed = tf.add(image_pixel, initial_noise)\n",
        "        # image_dashed_list.append(image_pixel_dashed)\n",
        "        # # image_dashed = np.array(image_dashed_list)\n",
        "        # image_dashed = tf.convert_to_tensor(image_dashed_list)\n",
        "        # print(image_dashed.numpy())\n",
        "\n",
        "        for j in range(iterate_num):\n",
        "          with tf.GradientTape() as tape:\n",
        "            tape.watch(image_dashed)\n",
        "            prediction = model(image_dashed, training = True)\n",
        "            loss = loss_object(label, prediction)\n",
        "          gradients = tape.gradient(loss, image_dashed)\n",
        "          image_med = tf.add(image_dashed, tf.multiply(eta1, gradients))\n",
        "          difference = tf.subtract(image_med, image)\n",
        "          if tf.norm(difference) <= eps1:\n",
        "            image_dashed = image_med \n",
        "          else:\n",
        "            image_dashed = difference\n",
        "            image_dashed = tf.multiply(image_dashed, eps1)\n",
        "            image_dashed = tf.divide(image_dashed, tf.norm(difference))\n",
        "            image_dashed = tf.add(image_dashed, image)\n",
        "        adversarial_image_list.append(image_dashed[0])\n",
        "        adversarial_image = np.array(adversarial_image_list)\n",
        "\n",
        "    return adversarial_image"
      ],
      "metadata": {
        "id": "V3j-1XQM1lMS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxll_dsBOldN"
      },
      "source": [
        "Step3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(dataset, batch_size = 1000):\n",
        "  batch_size = 1000\n",
        "  loss_sum = 0.0\n",
        "  for (images, labels) in dataset:\n",
        "    predictions = model(images, training = True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    loss_sum_batch = loss * batch_size\n",
        "    loss_sum = tf.add(loss_sum_batch, loss_sum)\n",
        "  average_loss = tf.divide(loss_sum, 1000)\n",
        "  return average_loss"
      ],
      "metadata": {
        "id": "ozxGyYUn3KGZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZ9Ze5LOldN"
      },
      "source": [
        "Step4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_weights_norm(weights):\n",
        "  w_flat = np.ndarray([])\n",
        "  for w in weights:\n",
        "    w_reshape = tf.reshape(w, [-1])\n",
        "    w_flat = np.hstack([w_flat, w_reshape])\n",
        "  w_fro = np.linalg.norm(w_flat)\n",
        "  return w_fro"
      ],
      "metadata": {
        "id": "BD3YeKDISlsZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatness(dataset, weights, v_fixed, eps2 = 1, eta2 = 0.1, batch_size = 1000, iterate_num = 5):\n",
        "  # vの初期値を，step2で固定したvに設定する．\n",
        "  v_updated = v_fixed\n",
        "\n",
        "  for i in range(iterate_num):\n",
        "\n",
        "    # 重みをw + (現在の)vに設定する　　ここから\n",
        "    current_weights = []\n",
        "    for layer_weights, v in zip(weights, v_updated):\n",
        "      current_layer_weight = tf.add(layer_weights, v)\n",
        "      current_weights.append(current_layer_weight)\n",
        "    model.set_weights(current_weights)\n",
        "    # 重みをw + (現在の)vに設定する　　ここまで\n",
        "\n",
        "    # gradientの計算　　ここから\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = calculate_loss(dataset)\n",
        "      # loss = tf.multiply(-1.0, calculate_loss(dataset))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #　計算できているし，悪い値ではない．\n",
        "    # gradientの計算　　ここまで\n",
        "\n",
        "    # gradientのノルムの計算　　ここから\n",
        "    gradients_flat = []\n",
        "    for g in gradients:\n",
        "      g_reshape = tf.reshape(g, [-1])\n",
        "      gradients_flat = np.hstack([gradients_flat, g_reshape])\n",
        "    gradients_norm = np.linalg.norm(gradients_flat)\n",
        "    # ループを経るごとに値が大きくなり過ぎている？\n",
        "    # gradientのノルムの計算　　ここまで\n",
        "\n",
        "    # 勾配降下の実行　　ここから\n",
        "    v_difference = []\n",
        "    for g, v in zip(gradients, v_updated):\n",
        "      g = tf.divide(g, gradients_norm)\n",
        "      g = tf.multiply(g, calculate_weights_norm(weights))\n",
        "      g = tf.multiply(eta2, g)\n",
        "      v_med = tf.add(v, g)\n",
        "    # 勾配降下の実行　　ここまで\n",
        "\n",
        "    # (現在の)vと(元の重み)wの差分と，　そのノルムを計算　　ここから\n",
        "      v_difference.append(v_med)\n",
        "    v_difference_flat = np.ndarray([])\n",
        "    for v_flat in v_difference:\n",
        "      v_flat_reshape = tf.reshape(v_flat, [-1])\n",
        "      v_difference_flat = np.hstack([v_difference_flat, v_flat_reshape])\n",
        "    v_difference_norm = np.linalg.norm(v_difference_flat)\n",
        "    # (現在の)vと(元の重み)wの差分と，　そのノルムを計算　　ここまで\n",
        "\n",
        "    # 射影の実行　　ここから\n",
        "    if v_difference_norm <= eps2:\n",
        "      v_hat = v_med \n",
        "    else:\n",
        "      v_hat = v_difference\n",
        "      v_hat = []\n",
        "      for v_hat_med in v_difference:\n",
        "        v_hat_med = tf.multiply(v_hat_med, eps2)\n",
        "        v_hat_med = tf.divide(v_hat_med, v_difference_norm)\n",
        "        v_hat.append(v_hat_med)\n",
        "      # for v_hat_med in v_hat:\n",
        "      #   v_hat_med = tf.multiply(v_hat_med, eps2)\n",
        "      #   v_hat_med = tf.divide(v_hat_med, v_difference_norm)\n",
        "    v_updated = v_hat\n",
        "    # 計算できているし，悪い値ではない．\n",
        "    # 射影の実行　　ここまで\n",
        "\n",
        "  return v_updated"
      ],
      "metadata": {
        "id": "7o_V5Ctgh40u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZjbQBHOldN"
      },
      "source": [
        "Step5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update(dataset, weights, v_updated, eta3 = 1):\n",
        "\n",
        "  # 重みをw + (step4で決めた)vに設定する　　ここから\n",
        "  current_weights = []\n",
        "  for layer_weights, v in zip(weights, v_updated):\n",
        "    current_layer_weight = tf.add(layer_weights, v)\n",
        "    current_weights.append(current_layer_weight)\n",
        "  model.set_weights(current_weights)\n",
        "  # 計算できている．\n",
        "  # 重みをw + (step4で決めた)vに設定する　　ここまで\n",
        "\n",
        "  # gradientの計算　　ここから\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = calculate_loss(dataset)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  # 計算できているが，値が大きい．\n",
        "  # gradientの計算　　ここまで\n",
        "\n",
        "  # 勾配降下の実行　　ここから\n",
        "  w_hat = []\n",
        "  for wu, g in zip(weights, gradients):\n",
        "    g_med = tf.multiply(eta3, g)\n",
        "    wu = tf.subtract(wu, g_med)\n",
        "    w_hat.append(wu)\n",
        "  weights_updated = w_hat\n",
        "  # 勾配降下の実行　　ここまで\n",
        "  return weights_updated"
      ],
      "metadata": {
        "id": "ZiCvT4zImjpo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTaFZYwFOldM"
      },
      "source": [
        "## AWPのアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AWP_execution(epochs):\n",
        "  result = []\n",
        "  for i in range(epochs):\n",
        "    result_i = []\n",
        "    result_i.append(i + 1)\n",
        "    # 重みをシフトして再設定する　ここから\n",
        "    ordinary_weights = model.get_weights()\n",
        "    v_array = []\n",
        "    for w in ordinary_weights:\n",
        "      v = 0.00002 * np.random.rand() - 0.00001\n",
        "      v_array.append(tf.fill(w.shape, v))\n",
        "    # 重みをシフトして再設定する　ここまで\n",
        "\n",
        "    # 敵対的データセットの生成　ここから\n",
        "    x_adversarial = adversary(train_ds, v_array, eps1 = 0.01, eta1 = 0.1, iterate_num = 5)\n",
        "    adversarial_ds = tf.data.Dataset.from_tensor_slices(\n",
        "        (x_adversarial, y_train)\n",
        "    ).batch(1000)\n",
        "    # 敵対的データセットの生成　ここまで\n",
        "\n",
        "    # ノイズとノイズ入りデータの可視化　ここから\n",
        "    # plt.imshow(np.squeeze(x_adversarial[0] - x_train[0]))\n",
        "    # plt.show()\n",
        "    # print(y_train[0])\n",
        "    # plt.imshow(np.squeeze(x_adversarial[0]))\n",
        "    # plt.show()\n",
        "    # print(y_train[0])\n",
        "    # ノイズとノイズ入りデータの可視化　ここまで\n",
        "\n",
        "    # 平坦度vの更新　ここから\n",
        "    new_v = flatness(adversarial_ds, ordinary_weights, v_array, eps2 = 0.01, eta2 = 0.1, batch_size = 1000, iterate_num = 5)\n",
        "    # 平坦度vの更新　ここまで\n",
        "\n",
        "    # 重みの更新　ここから\n",
        "    new_weights = update(adversarial_ds, ordinary_weights, new_v, eta3 = 0.1)\n",
        "    model.set_weights(new_weights)\n",
        "    # 重みの更新　ここまで\n",
        "\n",
        "    # テストの実行　ここから\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    for train_images, train_labels in train_ds:\n",
        "        train_result = test_step(train_images, train_labels)\n",
        "    result_i.append(test_loss.result())\n",
        "    result_i.append(test_accuracy.result() * 100)\n",
        "    # テストの実行　ここまで\n",
        "\n",
        "    # テストの実行　ここから\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_result = test_step(test_images, test_labels)\n",
        "    result_i.append(test_loss.result())\n",
        "    result_i.append(test_accuracy.result() * 100)\n",
        "    # テストの実行　ここまで\n",
        "\n",
        "    result.append(result_i)\n",
        "\n",
        "  return result\n",
        "  # print(\n",
        "  #       f'Epoch {epoch + 1}, '\n",
        "  #       f'Test Loss: {test_loss.result()}, '\n",
        "  #       f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "  #   )"
      ],
      "metadata": {
        "id": "PFohS-2CGB1a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "AWP = AWP_execution(epochs)\n",
        "for i in range(epochs):\n",
        "  print(\n",
        "        f'Epoch {AWP[i][0]}, '\n",
        "        f'Train Loss: {AWP[i][1]}, '\n",
        "        f'Train Accuracy: {AWP[i][2]}, '\n",
        "        f'Test Loss: {AWP[i][3]}, '\n",
        "        f'Test Accuracy: {AWP[i][4]}'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzZrJv63JbZE",
        "outputId": "b01fafa2-eb81-43f8-d0a8-df9e0fb94f16"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.26715442538261414, Train Accuracy: 93.4000015258789, Test Loss: 0.4279614984989166, Test Accuracy: 87.5\n",
            "Epoch 2, Train Loss: 0.27771803736686707, Train Accuracy: 92.29999542236328, Test Loss: 0.42572492361068726, Test Accuracy: 86.76000213623047\n",
            "Epoch 3, Train Loss: 0.3164493143558502, Train Accuracy: 91.9000015258789, Test Loss: 0.5117260217666626, Test Accuracy: 84.3499984741211\n",
            "Epoch 4, Train Loss: 0.41209009289741516, Train Accuracy: 86.4000015258789, Test Loss: 0.564509928226471, Test Accuracy: 81.91999816894531\n",
            "Epoch 5, Train Loss: 0.27390676736831665, Train Accuracy: 93.0, Test Loss: 0.443061500787735, Test Accuracy: 86.5\n",
            "Epoch 6, Train Loss: 0.25471970438957214, Train Accuracy: 92.79999542236328, Test Loss: 0.4089941382408142, Test Accuracy: 87.7699966430664\n",
            "Epoch 7, Train Loss: 0.2486531138420105, Train Accuracy: 92.9000015258789, Test Loss: 0.4105682969093323, Test Accuracy: 87.88999938964844\n",
            "Epoch 8, Train Loss: 0.24407416582107544, Train Accuracy: 92.79999542236328, Test Loss: 0.4069136083126068, Test Accuracy: 88.02000427246094\n",
            "Epoch 9, Train Loss: 0.24023552238941193, Train Accuracy: 92.9000015258789, Test Loss: 0.4065910279750824, Test Accuracy: 88.04000091552734\n",
            "Epoch 10, Train Loss: 0.23677100241184235, Train Accuracy: 93.0999984741211, Test Loss: 0.4041289687156677, Test Accuracy: 88.11000061035156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "AWP = AWP_execution(epochs)\n",
        "for i in range(epochs):\n",
        "  print(\n",
        "        f'Epoch {AWP[i][0]}, '\n",
        "        f'Train Loss: {AWP[i][1]}, '\n",
        "        f'Train Accuracy: {AWP[i][2]}, '\n",
        "        f'Test Loss: {AWP[i][3]}, '\n",
        "        f'Test Accuracy: {AWP[i][4]}'\n",
        "    )"
      ],
      "metadata": {
        "id": "s5G3VXapqNlM",
        "outputId": "4f7a45b5-46e3-4de5-b386-1c5ae2739a78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.23358486592769623, Train Accuracy: 93.4000015258789, Test Loss: 0.4034484028816223, Test Accuracy: 88.09000396728516\n",
            "Epoch 2, Train Loss: 0.23057128489017487, Train Accuracy: 93.30000305175781, Test Loss: 0.4011046886444092, Test Accuracy: 88.2300033569336\n",
            "Epoch 3, Train Loss: 0.22784917056560516, Train Accuracy: 93.4000015258789, Test Loss: 0.40066179633140564, Test Accuracy: 88.1500015258789\n",
            "Epoch 4, Train Loss: 0.2250533103942871, Train Accuracy: 93.5, Test Loss: 0.39700940251350403, Test Accuracy: 88.26000213623047\n",
            "Epoch 5, Train Loss: 0.22244638204574585, Train Accuracy: 93.69999694824219, Test Loss: 0.3983061611652374, Test Accuracy: 88.23999786376953\n",
            "Epoch 6, Train Loss: 0.22008489072322845, Train Accuracy: 93.5999984741211, Test Loss: 0.39354628324508667, Test Accuracy: 88.4000015258789\n",
            "Epoch 7, Train Loss: 0.21782752871513367, Train Accuracy: 93.80000305175781, Test Loss: 0.3970208168029785, Test Accuracy: 88.29000091552734\n",
            "Epoch 8, Train Loss: 0.21588394045829773, Train Accuracy: 93.69999694824219, Test Loss: 0.39037615060806274, Test Accuracy: 88.41999816894531\n",
            "Epoch 9, Train Loss: 0.2139347940683365, Train Accuracy: 93.80000305175781, Test Loss: 0.39631396532058716, Test Accuracy: 88.3800048828125\n",
            "Epoch 10, Train Loss: 0.2125956118106842, Train Accuracy: 94.19999694824219, Test Loss: 0.388141930103302, Test Accuracy: 88.47000122070312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "AWP = AWP_execution(epochs)\n",
        "for i in range(epochs):\n",
        "  print(\n",
        "        f'Epoch {AWP[i][0]}, '\n",
        "        f'Train Loss: {AWP[i][1]}, '\n",
        "        f'Train Accuracy: {AWP[i][2]}, '\n",
        "        f'Test Loss: {AWP[i][3]}, '\n",
        "        f'Test Accuracy: {AWP[i][4]}'\n",
        "    )"
      ],
      "metadata": {
        "id": "I-nhtN7qy6Fs",
        "outputId": "87aeec57-6e7c-44a4-af53-4a938dfd1000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.21132908761501312, Train Accuracy: 94.19999694824219, Test Loss: 0.3981184959411621, Test Accuracy: 88.20999908447266\n",
            "Epoch 2, Train Loss: 0.2116227000951767, Train Accuracy: 94.19999694824219, Test Loss: 0.38704732060432434, Test Accuracy: 88.38999938964844\n",
            "Epoch 3, Train Loss: 0.21114833652973175, Train Accuracy: 94.19999694824219, Test Loss: 0.40205734968185425, Test Accuracy: 88.12999725341797\n",
            "Epoch 4, Train Loss: 0.2131779044866562, Train Accuracy: 94.0, Test Loss: 0.3895135521888733, Test Accuracy: 88.30999755859375\n",
            "Epoch 5, Train Loss: 0.21510113775730133, Train Accuracy: 94.0, Test Loss: 0.4133935868740082, Test Accuracy: 87.70999908447266\n",
            "Epoch 6, Train Loss: 0.22216565907001495, Train Accuracy: 94.0999984741211, Test Loss: 0.39918798208236694, Test Accuracy: 87.98999786376953\n",
            "Epoch 7, Train Loss: 0.22015869617462158, Train Accuracy: 93.9000015258789, Test Loss: 0.4259805679321289, Test Accuracy: 87.29000091552734\n",
            "Epoch 8, Train Loss: 0.22634366154670715, Train Accuracy: 93.80000305175781, Test Loss: 0.40441837906837463, Test Accuracy: 87.7699966430664\n",
            "Epoch 9, Train Loss: 0.21943765878677368, Train Accuracy: 94.0999984741211, Test Loss: 0.429445743560791, Test Accuracy: 87.2300033569336\n",
            "Epoch 10, Train Loss: 0.2213924080133438, Train Accuracy: 93.69999694824219, Test Loss: 0.4003927707672119, Test Accuracy: 87.91999816894531\n",
            "Epoch 11, Train Loss: 0.2134293168783188, Train Accuracy: 94.4000015258789, Test Loss: 0.4248080849647522, Test Accuracy: 87.37999725341797\n",
            "Epoch 12, Train Loss: 0.21161213517189026, Train Accuracy: 94.0999984741211, Test Loss: 0.39229369163513184, Test Accuracy: 88.26000213623047\n",
            "Epoch 13, Train Loss: 0.20395657420158386, Train Accuracy: 94.5999984741211, Test Loss: 0.41268497705459595, Test Accuracy: 87.76000213623047\n",
            "Epoch 14, Train Loss: 0.20086699724197388, Train Accuracy: 94.5999984741211, Test Loss: 0.38459980487823486, Test Accuracy: 88.52000427246094\n",
            "Epoch 15, Train Loss: 0.1953524798154831, Train Accuracy: 94.70000457763672, Test Loss: 0.40171971917152405, Test Accuracy: 88.19000244140625\n",
            "Epoch 16, Train Loss: 0.1920991986989975, Train Accuracy: 94.80000305175781, Test Loss: 0.3784198462963104, Test Accuracy: 88.79000091552734\n",
            "Epoch 17, Train Loss: 0.18752871453762054, Train Accuracy: 94.5999984741211, Test Loss: 0.3928327262401581, Test Accuracy: 88.48999786376953\n",
            "Epoch 18, Train Loss: 0.1850368082523346, Train Accuracy: 94.9000015258789, Test Loss: 0.3749205768108368, Test Accuracy: 89.0\n",
            "Epoch 19, Train Loss: 0.18241527676582336, Train Accuracy: 94.5999984741211, Test Loss: 0.3879271149635315, Test Accuracy: 88.56999969482422\n",
            "Epoch 20, Train Loss: 0.18090423941612244, Train Accuracy: 95.0, Test Loss: 0.3733753561973572, Test Accuracy: 89.09000396728516\n",
            "Epoch 21, Train Loss: 0.177983820438385, Train Accuracy: 94.70000457763672, Test Loss: 0.38436150550842285, Test Accuracy: 88.73999786376953\n",
            "Epoch 22, Train Loss: 0.17569711804389954, Train Accuracy: 95.4000015258789, Test Loss: 0.37054145336151123, Test Accuracy: 89.20999908447266\n",
            "Epoch 23, Train Loss: 0.1734667271375656, Train Accuracy: 95.0, Test Loss: 0.38073939085006714, Test Accuracy: 88.88999938964844\n",
            "Epoch 24, Train Loss: 0.17194928228855133, Train Accuracy: 95.4000015258789, Test Loss: 0.3696165382862091, Test Accuracy: 89.24000549316406\n",
            "Epoch 25, Train Loss: 0.16976968944072723, Train Accuracy: 95.0, Test Loss: 0.3787020444869995, Test Accuracy: 88.94000244140625\n",
            "Epoch 26, Train Loss: 0.16803254187107086, Train Accuracy: 95.4000015258789, Test Loss: 0.3683749735355377, Test Accuracy: 89.27999877929688\n",
            "Epoch 27, Train Loss: 0.1664506047964096, Train Accuracy: 95.0, Test Loss: 0.37736776471138, Test Accuracy: 88.95999908447266\n",
            "Epoch 28, Train Loss: 0.16526183485984802, Train Accuracy: 95.5999984741211, Test Loss: 0.36796480417251587, Test Accuracy: 89.31999969482422\n",
            "Epoch 29, Train Loss: 0.16361086070537567, Train Accuracy: 95.0999984741211, Test Loss: 0.3765760660171509, Test Accuracy: 89.02999877929688\n",
            "Epoch 30, Train Loss: 0.16241155564785004, Train Accuracy: 95.5999984741211, Test Loss: 0.3669131398200989, Test Accuracy: 89.3800048828125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重みをシフトして再設定"
      ],
      "metadata": {
        "id": "J9V5IrZnDvUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights = model.get_weights()\n",
        "# ordinary_weights = weights\n",
        "# v_array = []\n",
        "# for w in weights:\n",
        "#   v = 0.00002 * np.random.rand() - 0.00001\n",
        "#   v_array.append(tf.fill(w.shape, v))"
      ],
      "metadata": {
        "id": "XRSISCKw7GT9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 敵対的データの生成"
      ],
      "metadata": {
        "id": "mfKKi-HCEIha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_adversarial = adversary(train_ds, v_array)"
      ],
      "metadata": {
        "id": "LzKpbHhf5dup"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adversarial_ds = tf.data.Dataset.from_tensor_slices(\n",
        "#     (x_adversarial, y_train)\n",
        "# ).batch(1000)"
      ],
      "metadata": {
        "id": "Cjf3frQs4Io4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ノイズとノイズ入り画像の可視化"
      ],
      "metadata": {
        "id": "cTZ21D6UEj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(np.squeeze(x_adversarial[0] - x_train[0]))\n",
        "# plt.show()\n",
        "# print(y_train[0])"
      ],
      "metadata": {
        "id": "aMZJZZYgA04d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(np.squeeze(x_adversarial[0]))\n",
        "# plt.show()\n",
        "# print(y_train[0])"
      ],
      "metadata": {
        "id": "6uaLXa-goe2o"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重みのフロベニウスノルムの計算"
      ],
      "metadata": {
        "id": "siB-Fe46E9FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# value1 = calculate_weights_norm(ordinary_weights)\n",
        "# print(value1)"
      ],
      "metadata": {
        "id": "hrmEYirzftOs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 平坦度vの更新"
      ],
      "metadata": {
        "id": "KUKMjCwRFHxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_v = flatness(adversarial_ds, ordinary_weights, v_array)"
      ],
      "metadata": {
        "id": "TPT1rqAXmFL7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重みの更新"
      ],
      "metadata": {
        "id": "70gvHkkrFShw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_weights = update(adversarial_ds, ordinary_weights, new_v)\n",
        "# print(new_weights)\n",
        "# model.set_weights(new_weights)\n",
        "# ordinary_weights = model.get_weights()"
      ],
      "metadata": {
        "id": "55nobDCBoXUt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3MclZYfOldN"
      },
      "source": [
        "再テストの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jAmhPk_tOldN"
      },
      "outputs": [],
      "source": [
        "# EPOCHS = 5\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     test_loss.reset_states()\n",
        "#     test_accuracy.reset_states()\n",
        "\n",
        "#     for test_images, test_labels in test_ds:\n",
        "#         test_step(test_images, test_labels)\n",
        "    \n",
        "#     print(\n",
        "#         f'Epoch {epoch + 1}, '\n",
        "#         f'Test Loss: {test_loss.result()}, '\n",
        "#         f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQmyQyMYKM4P"
      },
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('pythonenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d1685d247089585e01bacc0e11595c4779ea690fa157e920ca16120e3c1da301"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}