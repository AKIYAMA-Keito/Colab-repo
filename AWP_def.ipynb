{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKIYAMA-Keito/Colab-repo/blob/main/AWP_def.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d1Z8uIInOldG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKEv8KiMOldH"
      },
      "source": [
        "Step1 Tensorflowチュートリアル4から引用"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "version2\n"
      ],
      "metadata": {
        "id": "Yk9ofpRIRw2k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrpxk0rOldI"
      },
      "source": [
        "データセットを読み込んで正規化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QobaZNJyOldJ"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "random_index_train = np.array(range(len(x_train)))\n",
        "np.random.shuffle(random_index_train)\n",
        "x_train = x_train[random_index_train][:1000]\n",
        "y_train = y_train[random_index_train][:1000]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNcHvWE9OldJ"
      },
      "source": [
        "データセットをシャッフルしてバッチ化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wg2lQwV_OldJ"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)\n",
        ").batch(1000)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_test, y_test)\n",
        ").batch(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exQHK9sMOldK"
      },
      "source": [
        "CNNモデルを定義しインスタンスを取り出す．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1-UIMbDyOldK"
      },
      "outputs": [],
      "source": [
        "class CNNModel(Model):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = Conv2D(32, 3, activation = \"relu\")\n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(128, activation = \"relu\")\n",
        "        self.d2 = Dense(10)\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R46fxEjOOldK"
      },
      "source": [
        "損失関数とoptmizerを選択する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9pzC_gytOldL"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph_DSqV9OldL"
      },
      "source": [
        "損失関数とoptimizerの尺度評価のための関数を導入する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MipedjyLOldL"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6sZQRdOldL"
      },
      "source": [
        "モデルを訓練するための関数train_stepを定義する．\n",
        "予測値と正解ラベルの間の損失関数の勾配を最適化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KCmbB-cnOldL"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training = True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdL3GJY2OldM"
      },
      "source": [
        "モデルをテストするための関数test_stepを定義する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IsRpIQFEOldM"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "    predictions = model(images, training = False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "    \n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D-PUsW-OldM"
      },
      "source": [
        "学習を実行してテストし，結果を出力する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJEwX2bUOldM",
        "outputId": "4b0bff91-4237-4b7c-c58d-8914d76910c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.2853658199310303, Accuracy: 14.5, Test Loss: 1.8099327087402344, Test Accuracy: 68.58999633789062\n",
            "Epoch 2, Loss: 1.790489912033081, Accuracy: 70.0999984741211, Test Loss: 1.299665093421936, Test Accuracy: 75.05000305175781\n",
            "Epoch 3, Loss: 1.267777442932129, Accuracy: 76.0999984741211, Test Loss: 0.9724119901657104, Test Accuracy: 77.2800064086914\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    \n",
        "    for train_images, train_labels in train_ds:\n",
        "        train_step(train_images, train_labels)\n",
        "    \n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_step(test_images, test_labels)\n",
        "    \n",
        "    print(\n",
        "        f'Epoch {epoch + 1}, '\n",
        "        f'Loss: {train_loss.result()}, '\n",
        "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "        f'Test Loss: {test_loss.result()}, '\n",
        "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AWPのための関数"
      ],
      "metadata": {
        "id": "2H3oxHw4D5U8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPKvxUYsOldM"
      },
      "source": [
        "Step2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def adversary(dataset, additive_weights, eps1 = 1, eta1 = 0.1, iterate_num = 5):\n",
        "    weights = model.get_weights()\n",
        "    new_weights = []\n",
        "    for w, v in zip(weights, additive_weights):\n",
        "      new_weights.append(w + v)\n",
        "    model.set_weights(new_weights)\n",
        "\n",
        "    adversarial_image_list = []\n",
        "    for (images, labels) in dataset:\n",
        "      for (image, label) in zip(images, labels):\n",
        "        image = tf.Variable([image])\n",
        "\n",
        "        initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "        image_dashed = tf.add(image, initial_noise)\n",
        "\n",
        "        # この書き方でも，initial_noiseが一様に入ってしまっている．\n",
        "        # tf.Variableで1次元追加しているから，より内側のループを作らないといけないよ\n",
        "        # image_dashed_list = []\n",
        "        # for image_0 in image:\n",
        "        #   for image_h in image_0:\n",
        "        #     for image_v in image_h:\n",
        "        #       for image_pixel in image_v:\n",
        "        #         initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "        #         image_pixel_dashed = tf.add(image_pixel, initial_noise)\n",
        "        # image_dashed_list.append(image_pixel_dashed)\n",
        "        # # image_dashed = np.array(image_dashed_list)\n",
        "        # image_dashed = tf.convert_to_tensor(image_dashed_list)\n",
        "        # print(image_dashed.numpy())\n",
        "\n",
        "        for j in range(iterate_num):\n",
        "          with tf.GradientTape() as tape:\n",
        "            tape.watch(image_dashed)\n",
        "            prediction = model(image_dashed, training = True)\n",
        "            loss = loss_object(label, prediction)\n",
        "          gradients = tape.gradient(loss, image_dashed)\n",
        "          image_med = tf.add(image_dashed, tf.multiply(eta1, gradients))\n",
        "          difference = tf.subtract(image_med, image)\n",
        "          if tf.norm(difference) <= eps1:\n",
        "            image_dashed = image_med \n",
        "          else:\n",
        "            image_dashed = difference\n",
        "            image_dashed = tf.multiply(image_dashed, eps1)\n",
        "            image_dashed = tf.divide(image_dashed, tf.norm(difference))\n",
        "            image_dashed = tf.add(image_dashed, image)\n",
        "        adversarial_image_list.append(image_dashed[0])\n",
        "        adversarial_image = np.array(adversarial_image_list)\n",
        "\n",
        "    return adversarial_image"
      ],
      "metadata": {
        "id": "V3j-1XQM1lMS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxll_dsBOldN"
      },
      "source": [
        "Step3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(dataset, batch_size = 1000):\n",
        "  batch_size = 1000\n",
        "  loss_sum = 0.0\n",
        "  for (images, labels) in dataset:\n",
        "    predictions = model(images, training = True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    loss_sum_batch = loss * batch_size\n",
        "    loss_sum = tf.add(loss_sum_batch, loss_sum)\n",
        "  average_loss = tf.divide(loss_sum, 1000)\n",
        "  return average_loss"
      ],
      "metadata": {
        "id": "ozxGyYUn3KGZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZ9Ze5LOldN"
      },
      "source": [
        "Step4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_weights_norm(weights):\n",
        "  w_flat = np.ndarray([])\n",
        "  for w in weights:\n",
        "    w_reshape = tf.reshape(w, [-1])\n",
        "    w_flat = np.hstack([w_flat, w_reshape])\n",
        "  w_fro = np.linalg.norm(w_flat)\n",
        "  return w_fro"
      ],
      "metadata": {
        "id": "BD3YeKDISlsZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatness(dataset, weights, v_fixed, eps2 = 1, eta2 = 0.1, batch_size = 1000, iterate_num = 5):\n",
        "  # vの初期値を，step2で固定したvに設定する．\n",
        "  v_updated = v_fixed\n",
        "\n",
        "  for i in range(iterate_num):\n",
        "\n",
        "    # 重みをw + (現在の)vに設定する　　ここから\n",
        "    current_weights = []\n",
        "    for layer_weights, v in zip(weights, v_updated):\n",
        "      current_layer_weight = tf.add(layer_weights, v)\n",
        "      current_weights.append(current_layer_weight)\n",
        "    model.set_weights(current_weights)\n",
        "    # 重みをw + (現在の)vに設定する　　ここまで\n",
        "\n",
        "    # gradientの計算　　ここから\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = calculate_loss(dataset)\n",
        "      # loss = tf.multiply(-1.0, calculate_loss(dataset))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #　計算できているし，悪い値ではない．\n",
        "    # gradientの計算　　ここまで\n",
        "\n",
        "    # gradientのノルムの計算　　ここから\n",
        "    gradients_flat = []\n",
        "    for g in gradients:\n",
        "      g_reshape = tf.reshape(g, [-1])\n",
        "      gradients_flat = np.hstack([gradients_flat, g_reshape])\n",
        "    gradients_norm = np.linalg.norm(gradients_flat)\n",
        "    # ループを経るごとに値が大きくなり過ぎている？\n",
        "    # gradientのノルムの計算　　ここまで\n",
        "\n",
        "    # 勾配降下の実行　　ここから\n",
        "    v_difference = []\n",
        "    for g, v in zip(gradients, v_updated):\n",
        "      g = tf.divide(g, gradients_norm)\n",
        "      g = tf.multiply(g, calculate_weights_norm(weights))\n",
        "      g = tf.multiply(eta2, g)\n",
        "      v_med = tf.add(v, g)\n",
        "    # 勾配降下の実行　　ここまで\n",
        "\n",
        "    # (現在の)vと(元の重み)wの差分と，　そのノルムを計算　　ここから\n",
        "      v_difference.append(v_med)\n",
        "    v_difference_flat = np.ndarray([])\n",
        "    for v_flat in v_difference:\n",
        "      v_flat_reshape = tf.reshape(v_flat, [-1])\n",
        "      v_difference_flat = np.hstack([v_difference_flat, v_flat_reshape])\n",
        "    v_difference_norm = np.linalg.norm(v_difference_flat)\n",
        "    # (現在の)vと(元の重み)wの差分と，　そのノルムを計算　　ここまで\n",
        "\n",
        "    # 射影の実行　　ここから\n",
        "    if v_difference_norm <= eps2:\n",
        "      v_hat = v_med \n",
        "    else:\n",
        "      v_hat = v_difference\n",
        "      for v_hat_med in v_hat:\n",
        "        v_hat_med = tf.multiply(v_hat_med, eps2)\n",
        "        v_hat_med = tf.divide(v_hat_med, v_difference_norm)\n",
        "    v_updated = v_hat\n",
        "    # 計算できているし，悪い値ではない．\n",
        "    # 射影の実行　　ここまで\n",
        "\n",
        "  return v_updated"
      ],
      "metadata": {
        "id": "7o_V5Ctgh40u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZjbQBHOldN"
      },
      "source": [
        "Step5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update(dataset, weights, v_updated, eta3 = 1):\n",
        "\n",
        "  # 重みをw + (step4で決めた)vに設定する　　ここから\n",
        "  current_weights = []\n",
        "  for layer_weights, v in zip(weights, v_updated):\n",
        "    current_layer_weight = tf.add(layer_weights, v)\n",
        "    current_weights.append(current_layer_weight)\n",
        "  model.set_weights(current_weights)\n",
        "  # 計算できている．\n",
        "  # 重みをw + (step4で決めた)vに設定する　　ここまで\n",
        "\n",
        "  # gradientの計算　　ここから\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = calculate_loss(dataset)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  # 計算できているが，値が大きい．\n",
        "  # gradientの計算　　ここまで\n",
        "\n",
        "  # 勾配降下の実行　　ここから\n",
        "  w_hat = []\n",
        "  for wu, g in zip(weights, gradients):\n",
        "    g_med = tf.multiply(eta3, g)\n",
        "    wu = tf.subtract(wu, g_med)\n",
        "    w_hat.append(wu)\n",
        "  weights_updated = w_hat\n",
        "  # 勾配降下の実行　　ここまで\n",
        "  return weights_updated"
      ],
      "metadata": {
        "id": "ZiCvT4zImjpo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTaFZYwFOldM"
      },
      "source": [
        "## AWPのアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AWP_execution(epochs):\n",
        "  result = []\n",
        "  for i in range(epochs):\n",
        "    result_i = []\n",
        "    result_i.append(i + 1)\n",
        "    # 重みをシフトして再設定する　ここから\n",
        "    ordinary_weights = model.get_weights()\n",
        "    v_array = []\n",
        "    for w in ordinary_weights:\n",
        "      v = 0.00002 * np.random.rand() - 0.00001\n",
        "      v_array.append(tf.fill(w.shape, v))\n",
        "    # 重みをシフトして再設定する　ここまで\n",
        "\n",
        "    # 敵対的データセットの生成　ここから\n",
        "    x_adversarial = adversary(train_ds, v_array, eps1 = 0.01, eta1 = 0.1, iterate_num = 5)\n",
        "    adversarial_ds = tf.data.Dataset.from_tensor_slices(\n",
        "        (x_adversarial, y_train)\n",
        "    ).batch(1000)\n",
        "    # 敵対的データセットの生成　ここまで\n",
        "\n",
        "    # ノイズとノイズ入りデータの可視化　ここから\n",
        "    # plt.imshow(np.squeeze(x_adversarial[0] - x_train[0]))\n",
        "    # plt.show()\n",
        "    # print(y_train[0])\n",
        "    # plt.imshow(np.squeeze(x_adversarial[0]))\n",
        "    # plt.show()\n",
        "    # print(y_train[0])\n",
        "    # ノイズとノイズ入りデータの可視化　ここまで\n",
        "\n",
        "    # 平坦度vの更新　ここから\n",
        "    new_v = flatness(adversarial_ds, ordinary_weights, v_array, eps2 = 0.01, eta2 = 0.1, batch_size = 1000, iterate_num = 5)\n",
        "    # 平坦度vの更新　ここまで\n",
        "\n",
        "    # 重みの更新　ここから\n",
        "    new_weights = update(adversarial_ds, ordinary_weights, new_v, eta3 = 0.000001)\n",
        "    model.set_weights(new_weights)\n",
        "    # 重みの更新　ここまで\n",
        "\n",
        "    # テストの実行　ここから\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    for train_images, train_labels in train_ds:\n",
        "        train_result = test_step(train_images, train_labels)\n",
        "    result_i.append(test_loss.result())\n",
        "    result_i.append(test_accuracy.result() * 100)\n",
        "    # テストの実行　ここまで\n",
        "\n",
        "    # テストの実行　ここから\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_result = test_step(test_images, test_labels)\n",
        "    result_i.append(test_loss.result())\n",
        "    result_i.append(test_accuracy.result() * 100)\n",
        "    # テストの実行　ここまで\n",
        "\n",
        "    result.append(result_i)\n",
        "\n",
        "  return result\n",
        "  # print(\n",
        "  #       f'Epoch {epoch + 1}, '\n",
        "  #       f'Test Loss: {test_loss.result()}, '\n",
        "  #       f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "  #   )"
      ],
      "metadata": {
        "id": "PFohS-2CGB1a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "AWP = AWP_execution(epochs)\n",
        "for i in range(epochs):\n",
        "  print(\n",
        "        f'Epoch {AWP[i][0]}, '\n",
        "        f'Train Loss: {AWP[i][1]}, '\n",
        "        f'Train Accuracy: {AWP[i][2]}, '\n",
        "        f'Test Loss: {AWP[i][3]}, '\n",
        "        f'Test Accuracy: {AWP[i][4]}'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzZrJv63JbZE",
        "outputId": "7d3dcdf0-9e43-49a6-89fe-c9cbc2c7b8e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 21623.73046875, Train Accuracy: 10.300000190734863, Test Loss: 20916.09375, Test Accuracy: 9.739999771118164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重みをシフトして再設定"
      ],
      "metadata": {
        "id": "J9V5IrZnDvUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights = model.get_weights()\n",
        "# ordinary_weights = weights\n",
        "# v_array = []\n",
        "# for w in weights:\n",
        "#   v = 0.00002 * np.random.rand() - 0.00001\n",
        "#   v_array.append(tf.fill(w.shape, v))"
      ],
      "metadata": {
        "id": "XRSISCKw7GT9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 敵対的データの生成"
      ],
      "metadata": {
        "id": "mfKKi-HCEIha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_adversarial = adversary(train_ds, v_array)"
      ],
      "metadata": {
        "id": "LzKpbHhf5dup"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adversarial_ds = tf.data.Dataset.from_tensor_slices(\n",
        "#     (x_adversarial, y_train)\n",
        "# ).batch(1000)"
      ],
      "metadata": {
        "id": "Cjf3frQs4Io4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ノイズとノイズ入り画像の可視化"
      ],
      "metadata": {
        "id": "cTZ21D6UEj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(np.squeeze(x_adversarial[0] - x_train[0]))\n",
        "# plt.show()\n",
        "# print(y_train[0])"
      ],
      "metadata": {
        "id": "aMZJZZYgA04d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(np.squeeze(x_adversarial[0]))\n",
        "# plt.show()\n",
        "# print(y_train[0])"
      ],
      "metadata": {
        "id": "6uaLXa-goe2o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重みのフロベニウスノルムの計算"
      ],
      "metadata": {
        "id": "siB-Fe46E9FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# value1 = calculate_weights_norm(ordinary_weights)\n",
        "# print(value1)"
      ],
      "metadata": {
        "id": "hrmEYirzftOs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 平坦度vの更新"
      ],
      "metadata": {
        "id": "KUKMjCwRFHxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_v = flatness(adversarial_ds, ordinary_weights, v_array)"
      ],
      "metadata": {
        "id": "TPT1rqAXmFL7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重みの更新"
      ],
      "metadata": {
        "id": "70gvHkkrFShw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_weights = update(adversarial_ds, ordinary_weights, new_v)\n",
        "# print(new_weights)\n",
        "# model.set_weights(new_weights)\n",
        "# ordinary_weights = model.get_weights()"
      ],
      "metadata": {
        "id": "55nobDCBoXUt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3MclZYfOldN"
      },
      "source": [
        "再テストの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jAmhPk_tOldN"
      },
      "outputs": [],
      "source": [
        "# EPOCHS = 5\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     test_loss.reset_states()\n",
        "#     test_accuracy.reset_states()\n",
        "\n",
        "#     for test_images, test_labels in test_ds:\n",
        "#         test_step(test_images, test_labels)\n",
        "    \n",
        "#     print(\n",
        "#         f'Epoch {epoch + 1}, '\n",
        "#         f'Test Loss: {test_loss.result()}, '\n",
        "#         f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQmyQyMYKM4P"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('pythonenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d1685d247089585e01bacc0e11595c4779ea690fa157e920ca16120e3c1da301"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}