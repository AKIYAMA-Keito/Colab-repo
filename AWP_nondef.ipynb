{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKIYAMA-Keito/Colab-repo/blob/main/AWP_nondef.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1Z8uIInOldG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKEv8KiMOldH"
      },
      "source": [
        "Step1 Tensorflowチュートリアル4から引用"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "version2\n"
      ],
      "metadata": {
        "id": "Yk9ofpRIRw2k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrpxk0rOldI"
      },
      "source": [
        "データセットを読み込んで正規化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QobaZNJyOldJ"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "random_index_train = np.array(range(len(x_train)))\n",
        "np.random.shuffle(random_index_train)\n",
        "x_train = x_train[random_index_train][:1000]\n",
        "y_train = y_train[random_index_train][:1000]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test/255.0\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNcHvWE9OldJ"
      },
      "source": [
        "データセットをシャッフルしてバッチ化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg2lQwV_OldJ"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)\n",
        ").batch(1000)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_test, y_test)\n",
        ").batch(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exQHK9sMOldK"
      },
      "source": [
        "CNNモデルを定義しインスタンスを取り出す．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-UIMbDyOldK"
      },
      "outputs": [],
      "source": [
        "class CNNModel(Model):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = Conv2D(32, 3, activation = \"relu\")\n",
        "        self.flatten = Flatten()\n",
        "        self.d1 = Dense(128, activation = \"relu\")\n",
        "        self.d2 = Dense(10)\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R46fxEjOOldK"
      },
      "source": [
        "損失関数とoptmizerを選択する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pzC_gytOldL"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph_DSqV9OldL"
      },
      "source": [
        "損失関数とoptimizerの尺度評価のための関数を導入する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MipedjyLOldL"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6sZQRdOldL"
      },
      "source": [
        "モデルを訓練するための関数train_stepを定義する．\n",
        "予測値と正解ラベルの間の損失関数の勾配を最適化する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCmbB-cnOldL"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training = True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdL3GJY2OldM"
      },
      "source": [
        "モデルをテストするための関数test_stepを定義する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsRpIQFEOldM"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "    predictions = model(images, training = False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "    \n",
        "    test_loss(t_loss)\n",
        "    test_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D-PUsW-OldM"
      },
      "source": [
        "学習を実行してテストし，結果を出力する．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJEwX2bUOldM"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    \n",
        "    for train_images, train_labels in train_ds:\n",
        "        train_step(train_images, train_labels)\n",
        "    \n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_step(test_images, test_labels)\n",
        "    \n",
        "    print(\n",
        "        f'Epoch {epoch + 1}, '\n",
        "        f'Loss: {train_loss.result()}, '\n",
        "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "        f'Test Loss: {test_loss.result()}, '\n",
        "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTaFZYwFOldM"
      },
      "source": [
        "AWPのアルゴリズムを記述する．\n",
        "まず，学習済みのmodelから重みを取り出してvを加えて再設定する．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_weights()\n",
        "ordinary_weights = weights\n",
        "v_array = []\n",
        "for w in weights:\n",
        "  v = 0.02 * np.random.rand() - 0.01\n",
        "  v_array.append(tf.fill(w.shape, v))"
      ],
      "metadata": {
        "id": "XRSISCKw7GT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPKvxUYsOldM"
      },
      "source": [
        "Step2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def adversary(dataset, additive_weights, eps1 = 1, eta1 = 0.1):\n",
        "    weights = model.get_weights()\n",
        "    new_weights = []\n",
        "    for w, v in zip(weights, additive_weights):\n",
        "      new_weights.append(w + v)\n",
        "    model.set_weights(new_weights)\n",
        "\n",
        "    adversarial_image_list = []\n",
        "    # eps1 = 1\n",
        "    # eta1 = 0.1\n",
        "    for (images, labels) in dataset:\n",
        "      for (image, label) in zip(images, labels):\n",
        "        image = tf.Variable([image])\n",
        "\n",
        "        initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "        image_dashed = tf.add(image, initial_noise)\n",
        "\n",
        "        # この書き方でも，initial_noiseが一様に入ってしまっている．\n",
        "        # tf.Variableで1次元追加しているから，より内側のループを作らないといけないよ\n",
        "        # image_dashed_list = []\n",
        "        # for image_0 in image:\n",
        "        #   for image_h in image_0:\n",
        "        #     for image_v in image_h:\n",
        "        #       for image_pixel in image_v:\n",
        "        #         initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "        #         image_pixel_dashed = tf.add(image_pixel, initial_noise)\n",
        "        # image_dashed_list.append(image_pixel_dashed)\n",
        "        # # image_dashed = np.array(image_dashed_list)\n",
        "        # image_dashed = tf.convert_to_tensor(image_dashed_list)\n",
        "        # print(image_dashed.numpy())\n",
        "\n",
        "        for j in range(5):\n",
        "          with tf.GradientTape() as tape:\n",
        "            tape.watch(image_dashed)\n",
        "            prediction = model(image_dashed, training = True)\n",
        "            loss = loss_object(label, prediction)\n",
        "          gradients = tape.gradient(loss, image_dashed)\n",
        "          image_med = tf.add(image_dashed, tf.multiply(eta1, gradients))\n",
        "          difference = tf.subtract(image_med, image)\n",
        "          if tf.norm(difference) <= eps1:\n",
        "            image_dashed = image_med \n",
        "          else:\n",
        "            image_dashed = difference\n",
        "            image_dashed = tf.multiply(image_dashed, eps1)\n",
        "            image_dashed = tf.divide(image_dashed, tf.norm(difference))\n",
        "            image_dashed = tf.add(image_dashed, image)\n",
        "        adversarial_image_list.append(image_dashed[0])\n",
        "        adversarial_image = np.array(adversarial_image_list)\n",
        "\n",
        "    return adversarial_image"
      ],
      "metadata": {
        "id": "V3j-1XQM1lMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # def adversary(dataset, additive_weights, eps1 = 1, eta1 = 0.1):\n",
        "# weights = model.get_weights()\n",
        "# new_weights = []\n",
        "# for w, v in zip(weights, v_array):\n",
        "#   new_weights.append(w + v)\n",
        "# model.set_weights(new_weights)\n",
        "\n",
        "# adversarial_image_list = []\n",
        "# eps1 = 1\n",
        "# eta1 = 0.1\n",
        "# for (images, labels) in train_ds:\n",
        "#   for (image, label) in zip(images, labels):\n",
        "#     image = tf.Variable([image])\n",
        "\n",
        "#     initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "#     image_dashed = tf.add(image, initial_noise)\n",
        "#     # print(image)\n",
        "#     # print(image_dashed)\n",
        "\n",
        "#     # この書き方でも，initial_noiseが一様に入ってしまっている．\n",
        "#     # tf.Variableで1次元追加しているから，より内側のループを作らないといけないよ\n",
        "#     # image_dashed_list = []\n",
        "#     # for image_0 in image:\n",
        "#     #   for image_h in image_0:\n",
        "#     #     for image_v in image_h:\n",
        "#     #       for image_pixel in image_v:\n",
        "#     #         initial_noise = 2 * eps1 * np.random.rand() - eps1\n",
        "#     #         image_pixel_dashed = tf.add(image_pixel, initial_noise)\n",
        "#     # image_dashed_list.append(image_pixel_dashed)\n",
        "#     # # image_dashed = np.array(image_dashed_list)\n",
        "#     # image_dashed = tf.convert_to_tensor(image_dashed_list)\n",
        "#     # print(image_dashed.numpy())\n",
        "\n",
        "#     for j in range(1):\n",
        "#       with tf.GradientTape() as tape:\n",
        "#         tape.watch(image_dashed)\n",
        "#         prediction = model(image_dashed, training = False)\n",
        "#         # print(prediction)\n",
        "#         loss = loss_object(label, prediction)\n",
        "#       gradients = tape.gradient(loss, image_dashed)\n",
        "#       image_med = tf.add(image_dashed, tf.multiply(eta1, gradients))\n",
        "#       difference = tf.subtract(image_med, image)\n",
        "#       if tf.norm(difference) <= eps1:\n",
        "#         image_dashed = image_med \n",
        "#       else:\n",
        "#         image_dashed = difference\n",
        "#         image_dashed = tf.multiply(image_dashed, eps1)\n",
        "#         image_dashed = tf.divide(image_dashed, tf.norm(difference))\n",
        "#         image_dashed = tf.add(image_dashed, image)\n",
        "#     adversarial_image_list.append(image_dashed[0])\n",
        "#     adversarial_image = np.array(adversarial_image_list)\n",
        "\n",
        "# print(adversarial_image[0] - x_train[0])\n",
        "\n",
        "# # print(adversarial_image)"
      ],
      "metadata": {
        "id": "2HGcWwIkljvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adversarial = adversary(train_ds, v_array)"
      ],
      "metadata": {
        "id": "LzKpbHhf5dup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mpzb3j_c8kKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(10):\n",
        "plt.imshow(np.squeeze(x_adversarial[0] - x_train[0]))\n",
        "plt.show()\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "aMZJZZYgA04d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.squeeze(x_adversarial[0]))\n",
        "plt.show()\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "6uaLXa-goe2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %matplotlib inline\n",
        "# for i in range(10):\n",
        "#   plt.imshow(np.squeeze(x_adversarial[i]))\n",
        "#   plt.show()\n",
        "#   print(y_train[i])"
      ],
      "metadata": {
        "id": "8C5mNp328y8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxll_dsBOldN"
      },
      "source": [
        "Step3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adversarial_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_adversarial, y_train)\n",
        ").batch(100)"
      ],
      "metadata": {
        "id": "Cjf3frQs4Io4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(dataset, batch_size = 100):\n",
        "  batch_size = 100\n",
        "  loss_sum = 0.0\n",
        "  for (images, labels) in adversarial_ds:\n",
        "    predictions = model(images, training = True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    loss_sum_batch = loss * batch_size\n",
        "    loss_sum = tf.add(loss_sum_batch, loss_sum)\n",
        "  average_loss = tf.divide(loss_sum, 1000)\n",
        "  return average_loss"
      ],
      "metadata": {
        "id": "ozxGyYUn3KGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 100\n",
        "# loss_sum = 0.0\n",
        "# for (images, labels) in adversarial_ds:\n",
        "#   predictions = model(images, training = True)\n",
        "#   loss = loss_object(labels, predictions)\n",
        "#   loss_sum_batch = loss * batch_size\n",
        "#   loss_sum = tf.add(loss_sum_batch, loss_sum)\n",
        "# average_loss = tf.divide(loss_sum, 1000)"
      ],
      "metadata": {
        "id": "rD2V2J5I5E1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_loss(dataset, batch_size = 100, additive_noise = v_array):\n",
        "#   # batch_size = 100\n",
        "#   loss_sum = 0.0\n",
        "#   for (images, labels) in dataset:\n",
        "#     predictions = model(images, training = True)\n",
        "#     loss = loss_object(labels, predictions)\n",
        "#     loss_sum_batch = loss * batch_size\n",
        "#     loss_sum = tf.add(loss_sum_batch, loss_sum)\n",
        "#   average_loss = tf.divide(loss_sum, 1000)\n",
        "#   return average_loss"
      ],
      "metadata": {
        "id": "H1C7Lp-g3JzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZ9Ze5LOldN"
      },
      "source": [
        "Step4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_flat = np.ndarray([])\n",
        "for w in ordinary_weights:\n",
        "  w_reshape = tf.reshape(w, [-1])\n",
        "  print(w_reshape.shape)\n",
        "  w_flat = np.hstack([w_flat, w_reshape])\n",
        "w_fro = np.linalg.norm(w_flat)\n",
        "print(w_fro)\n",
        "  # w_fro = tf.add(w_fro, np.linalg.norm(w_reshape, ord=2))\n",
        "# w_fro = tf.sqrt(w_fro)"
      ],
      "metadata": {
        "id": "BD3YeKDISlsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.rcsetup import validate_axisbelow\n",
        "eps2 = 1\n",
        "eta2 = 0.1\n",
        "batch_size = 100\n",
        "\n",
        "v_updated = v_array\n",
        "# vの初期値を，step2で固定したvに設定する．\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  # 重みをw + (現在の)vに設定する　　ここから\n",
        "  current_weights = []\n",
        "  for layer_weights, v in zip(ordinary_weights, v_updated):\n",
        "    current_layer_weight = tf.add(layer_weights, v)\n",
        "    current_weights.append(current_layer_weight)\n",
        "  model.set_weights(current_weights)\n",
        "  # 重みをw + (現在の)vに設定する　　ここまで\n",
        "\n",
        "  # gradientの計算　　ここから\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = calculate_loss(adversarial_ds)\n",
        "    # print(loss)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  print(gradients)\n",
        "  #　計算できているし，悪い値ではない．\n",
        "  # gradientの計算　　ここまで\n",
        "\n",
        "  # gradientのノルムの計算　　ここから\n",
        "  gradients_flat = np.ndarray([])\n",
        "  for g in gradients:\n",
        "    g_reshape = tf.reshape(g, [-1])\n",
        "    gradients_flat = np.hstack([gradients_flat, g_reshape])\n",
        "  gradients_norm = np.linalg.norm(gradients_flat)\n",
        "  print(gradients_flat)\n",
        "  print(gradients_norm)\n",
        "  # ループを経るごとに値が大きくなり過ぎている？\n",
        "  # gradientのノルムの計算　　ここまで\n",
        "\n",
        "  # 勾配降下の実行　　ここから\n",
        "  v_difference = []\n",
        "  for g, v in zip(gradients, v_updated):\n",
        "    g = tf.divide(g, gradients_norm)\n",
        "    g = tf.multiply(g, w_fro)\n",
        "    g = tf.multiply(eta2, g)\n",
        "    v_med = tf.add(v, g)\n",
        "  # 勾配降下の実行　　ここまで\n",
        "\n",
        "  # (現在の)vと(元の重み)wの差分と，　そのノルムを計算　　ここから\n",
        "    v_difference.append(v_med)\n",
        "  v_difference_flat = np.ndarray([])\n",
        "  for v_flat in v_difference:\n",
        "    v_flat_reshape = tf.reshape(v_flat, [-1])\n",
        "    # print(g_reshape.shape)\n",
        "    v_difference_flat = np.hstack([v_difference_flat, v_flat_reshape])\n",
        "  v_difference_norm = np.linalg.norm(v_difference_flat)\n",
        "  # (現在の)vと(元の重み)wの差分と，　そのノルムを計算　　ここまで\n",
        "\n",
        "  # 射影の実行　　ここから\n",
        "  if v_difference_norm <= eps2:\n",
        "    v_hat = v_med \n",
        "  else:\n",
        "    v_hat = v_difference\n",
        "    for v_hat_med in v_hat:\n",
        "      v_hat_med = tf.multiply(v_hat_med, eps2)\n",
        "      v_hat_med = tf.divide(v_hat_med, v_difference_norm)\n",
        "  v_updated = v_hat\n",
        "  # print(v_updated)　計算できているし，悪い値ではない．\n",
        "  # 射影の実行　　ここまで"
      ],
      "metadata": {
        "id": "dhQbeCasjVRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ZjbQBHOldN"
      },
      "source": [
        "Step5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eta3 = 1\n",
        "\n",
        "weights_updated = ordinary_weights\n",
        "for i in range(1):\n",
        "\n",
        "  # 重みをw + (step4で決めた)vに設定する　　ここから\n",
        "  current_weights = []\n",
        "  for layer_weights, v in zip(weights_updated, v_updated):\n",
        "    current_layer_weight = tf.add(layer_weights, v)\n",
        "    current_weights.append(current_layer_weight)\n",
        "  model.set_weights(current_weights)\n",
        "  print(current_weights) \n",
        "  # 計算できている．\n",
        "  # 重みをw + (step4で決めた)vに設定する　　ここまで\n",
        "\n",
        "  # gradientの計算　　ここから\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = calculate_loss(adversarial_ds)\n",
        "    print(loss)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  # print(gradients) \n",
        "  # 計算できているが，値が大きい．\n",
        "  # gradientの計算　　ここまで\n",
        "\n",
        "  # 勾配降下の実行　　ここから\n",
        "  w_hat = []\n",
        "  for wu, g in zip(weights_updated, gradients):\n",
        "    g_med = tf.multiply(eta3, g)\n",
        "    wu = tf.subtract(wu, g_med)\n",
        "    w_hat.append(wu)\n",
        "  weights_updated = w_hat\n",
        "  # 勾配降下の実行　　ここまで"
      ],
      "metadata": {
        "id": "_0bd9iyoK9EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3MclZYfOldN"
      },
      "source": [
        "再テストの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAmhPk_tOldN"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "\n",
        "    for test_images, test_labels in test_ds:\n",
        "        test_step(test_images, test_labels)\n",
        "    \n",
        "    print(\n",
        "        f'Epoch {epoch + 1}, '\n",
        "        f'Test Loss: {test_loss.result()}, '\n",
        "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQmyQyMYKM4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('pythonenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d1685d247089585e01bacc0e11595c4779ea690fa157e920ca16120e3c1da301"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}